{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding \n",
    "\n",
    "# Problem Definition \n",
    "\n",
    "## Context\n",
    "\n",
    "Vaccination is a cornerstone of public health, protecting individuals and communities through both direct immunity and herd immunity. Insights from past campaigns, such as the 2009 H1N1 influenza pandemic, can guide current and future vaccination efforts, including for emerging diseases like COVID-19. Understanding factors influencing vaccine uptake helps public health authorities design targeted interventions to improve coverage.\n",
    "\n",
    "### Stakeholder\n",
    "Public health authorities, such as the CDC, are responsible for monitoring vaccine coverage and planning immunization campaigns. Predictive insights from this analysis can help identify populations less likely to get vaccinated and guide resource allocation.\n",
    "\n",
    "### Problem Statement\n",
    "This project aims to predict whether an individual received the H1N1 or seasonal flu vaccine based on demographic, behavioral, and opinion-based survey responses. By modeling vaccination likelihood, I can:\n",
    "\n",
    "1. Identify key factors associated with vaccine uptake.\n",
    "2. Predict vaccination probability for individuals or populations.\n",
    "3. Inform targeted public health messaging and interventions.\n",
    "\n",
    "### Scope and Evaluation\n",
    "The analysis focuses on one vaccine type (H1N1 or seasonal) and includes:\n",
    "1. Exploratory Data Analysis (EDA) to understand distributions and relationships.\n",
    "2. Feature engineering and preprocessing to prepare the data for modeling.\n",
    "3. Model training using classification algorithms.\n",
    "4. Evaluation using accuracy, precision, recall, F1-score, and primarily ROC-AUC to measure predictive performance.\n",
    "\n",
    "### Business Value\n",
    "Predictive insights enable public health officials to:\n",
    "1. Identify groups with lower vaccination rates.\n",
    "2. Design more effective, data-driven vaccination campaigns.\n",
    "3. Allocate resources efficiently to increase overall vaccine coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files from the data folder\n",
    "train_features = pd.read_csv('data/training_set_features.csv')\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('data/training_set_labels.csv')\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = pd.read_csv('data/test_set_features.csv')\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_format = pd.read_csv('data/submission_format.csv')\n",
    "submission_format.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(submission_format.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "train_features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "train_features.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target distribution\n",
    "train_labels['h1n1_vaccine'].value_counts()\n",
    "train_labels['seasonal_vaccine'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features['household_adults'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='age_group', data=train_features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick statistics\n",
    "train_features.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Features and Labels\n",
    "\n",
    "Combines survey features and vaccination outcomes into a single dataframe. \n",
    "This simplifies preprocessing and EDA because all info is in one table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging training_set_features.csv and training_set_labels.csv into one dataframe for cleaning and EDA.\n",
    "# Merge on respondent_id\n",
    "train = train_features.merge(train_labels, on='respondent_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Column Names\n",
    "\n",
    "Removes spaces, converts names to lowercase, and standardizes naming. \n",
    "This ensures your code won’t break due to typos or inconsistent naming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove whitespaces\n",
    "# Clean column names\n",
    "train.columns = train.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Categorical Values\n",
    "\n",
    "Converts all categorical entries to lowercase strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning String/Category Values\n",
    "# Identify categorical columns (example)\n",
    "categorical_cols = ['age_group', 'education', 'race', 'sex', 'income_poverty', \n",
    "                    'marital_status', 'rent_or_own', 'employment_status', \n",
    "                    'hhs_geo_region', 'census_msa', 'employment_industry', \n",
    "                    'employment_occupation']\n",
    "\n",
    "# Clean string values\n",
    "for col in categorical_cols:\n",
    "    train[col] = train[col].astype(str).str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values in categorical\n",
    "Replaces missing values (NaN) with \"unknown\" to handle missing data without dropping rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling Missing Values\n",
    "# Fill missing categorical values\n",
    "for col in categorical_cols:\n",
    "    train[col] = train[col].replace('nan', 'unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values in numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing numeric values with median\n",
    "numeric_cols = train.select_dtypes(include='number').columns\n",
    "train[numeric_cols] = train[numeric_cols].fillna(train[numeric_cols].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Features\n",
    "Converts categories into numerical columns (one-hot encoding). drop_first=True avoids collinearity problems in models like Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Categorical Features\n",
    "# One-hot encoding for categorical variables\n",
    "# drop_first=True avoids multicollinearity for linear models\n",
    "train_encoded = pd.get_dummies(train, columns=categorical_cols, drop_first=True)\n",
    "train_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaning test_set_features.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names\n",
    "test_features.columns = test_features.columns.str.strip().str.lower().str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean string values\n",
    "for col in categorical_cols:\n",
    "    test_features[col] = test_features[col].astype(str).str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing categorical values\n",
    "for col in categorical_cols:\n",
    "    test_features[col] = test_features[col].replace('nan', 'unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill numeric missing values\n",
    "numeric_cols_test = test_features.select_dtypes(include='number').columns\n",
    "test_features[numeric_cols_test] = test_features[numeric_cols_test].fillna(test_features[numeric_cols_test].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "test_encoded = pd.get_dummies(test_features, columns=categorical_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "\n",
    "Ensures the test set has exactly the same features as the training set. \n",
    "Missing dummy columns in the test set are filled with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Align train and test columns\n",
    "train_encoded, test_encoded = train_encoded.align(test_encoded, join='left', axis=1, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify shapes\n",
    "print(train_encoded.shape)\n",
    "print(test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "# Make a copy of the dataframe\n",
    "df = test_encoded.copy()\n",
    "# Check the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "visualize the correlation between features in your dataset using a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute correlation matrix\n",
    "corr = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask weak correlations (keep |corr| > 0.7)\n",
    "strong_corr = corr.copy()\n",
    "strong_corr[(corr > -0.7) & (corr < 0.7)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(\n",
    "    strong_corr, \n",
    "    annot=True, \n",
    "    fmt=\".2f\", \n",
    "    cmap=\"coolwarm\",  \n",
    "    center=0,          \n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\":0.7}\n",
    ")\n",
    "plt.title(\"Strong Correlations (>|0.7|)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List feature pairs with strong correlations\n",
    "corr_pairs = corr.abs().unstack()  # flatten matrix\n",
    "# Remove self-correlations\n",
    "corr_pairs = corr_pairs[corr_pairs < 1]\n",
    "# Keep only correlations > 0.7\n",
    "high_corr = corr_pairs[corr_pairs > 0.7].sort_values(ascending=False)\n",
    "print(\"Highly correlated feature pairs:\\n\", high_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: get a set of features to drop\n",
    "# (keep one feature per highly correlated pair)\n",
    "to_drop = set()\n",
    "for feat1, feat2 in high_corr.index:\n",
    "    # choose one to drop (example: drop the second feature)\n",
    "    to_drop.add(feat2)\n",
    "\n",
    "print(\"\\nSuggested features to drop due to high correlation:\\n\", to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concise stat\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check unique values\n",
    "for i in df.columns:\n",
    "    uniq_val = df[i].unique()\n",
    "    print(f'Column name: {i}\\n, {uniq_val}\\n')\n",
    "    print(\"****\"*20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variables\n",
    "target = 'h1n1_vaccine'  # or 'seasonal_vaccine'\n",
    "X = train_encoded.drop(['h1n1_vaccine', 'seasonal_vaccine', 'respondent_id'], axis=1)\n",
    "y = train_encoded[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split data\n",
    "\n",
    "Creates training and validation sets. \n",
    "stratify=y ensures the proportion of vaccinated vs non-vaccinated is the same in both sets, preventing bias in evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Target variable: {target}\")\n",
    "print(f\"Training samples: {X_train.shape[0]}, Validation samples: {X_val.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape , y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scale the features\n",
    "\n",
    "Thecode ensures that all your features are on the same scale, which improves model stability, convergence, and performance, especially for algorithms sensitive to feature magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Scale the features\n",
    "X_train_s = sc.fit_transform(X_train)  # fit on training data\n",
    "X_val_s = sc.transform(X_val)          # transform validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: check shapes\n",
    "print(X_train_s.shape, X_val_s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model: Logistic Regression\n",
    "\n",
    "Baseline model using a simple, interpretable algorithm.\n",
    "Predicts probability of vaccination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check training score\n",
    "train_score = model1.score(x_train_s, y_train)\n",
    "print(f'{train_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#made prediction\n",
    "# Predictions\n",
    "y_pred_log = log_model.predict(X_val)\n",
    "y_pred_proba_log = log_model.predict_proba(X_val)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "roc_auc_log = roc_auc_score(y_val, y_pred_proba_log)\n",
    "print(\"Logistic Regression ROC-AUC:\", round(roc_auc_log, 3))\n",
    "print(classification_report(y_val, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the r2 error\n",
    "round(r2_score(y_test, y_pred),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Model: Random Forest (Tuned)\n",
    "\n",
    "Random Forest is an ensemble model that handles nonlinearities and interactions.\n",
    "GridSearchCV tunes hyperparameters to maximize ROC-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your single target for the MVP\n",
    "target = 'h1n1_vaccine'\n",
    "\n",
    "# Define X and y\n",
    "X = train_encoded.drop(['h1n1_vaccine', 'seasonal_vaccine', 'respondent_id'], axis=1)\n",
    "y = train_encoded[target]\n",
    "\n",
    "# Check alignment\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# Split train/test with matching rows\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(\n",
    "    rf, param_grid, scoring='roc_auc', cv=3, n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "y_proba_rf = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# ROC-AUC\n",
    "roc_auc_rf = roc_auc_score(y_test, y_proba_rf)\n",
    "print(f\"Random Forest ROC-AUC: {roc_auc_rf:.3f}\\n\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_estimator(best_rf, X_test, y_test, display_labels=[0, 1], cmap='Blues')\n",
    "plt.title(\"Random Forest Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "Identifies which survey questions and demographic factors most influence vaccination prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract feature importance\n",
    "feat_imp = pd.Series(best_rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "# Show top 15 features\n",
    "top_features = feat_imp.head(15)\n",
    "print(\"\\nTop 15 Important Features:\\n\", top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot from the feature importance\n",
    "top_features.plot(kind='barh', figsize=(8,6), color='teal')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Top 15 Important Features - Random Forest\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features once\n",
    "feature_cols = train_encoded.drop(['respondent_id','h1n1_vaccine','seasonal_vaccine'], axis=1).columns\n",
    "X_train = train_encoded[feature_cols]\n",
    "X_test_final = test_encoded[feature_cols]\n",
    "\n",
    "# Targets\n",
    "y_train = train_encoded[['h1n1_vaccine','seasonal_vaccine']]\n",
    "\n",
    "# Split\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "# Multi-output Logistic Regression\n",
    "model = MultiOutputClassifier(LogisticRegression(max_iter=500, random_state=42))\n",
    "model.fit(X_tr, y_tr)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_val_pred = model.predict_proba(X_val)\n",
    "auc_h1n1 = roc_auc_score(y_val['h1n1_vaccine'], y_val_pred[0][:,1])\n",
    "auc_seasonal = roc_auc_score(y_val['seasonal_vaccine'], y_val_pred[1][:,1])\n",
    "print(\"ROC AUC H1N1:\", auc_h1n1)\n",
    "print(\"ROC AUC Seasonal:\", auc_seasonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi variable Logistic Regression\n",
    "Simultaneously predicts H1N1 and seasonal flu vaccination probabilities.\n",
    "Produces probabilities, not just 0/1 predictions → useful for risk stratification and campaign targeting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi-label binary classification\n",
    "model = MultiOutputClassifier(LogisticRegression(max_iter=500))\n",
    "model.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_val_pred = model.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC AUC for each target\n",
    "auc_h1n1 = roc_auc_score(y_val['h1n1_vaccine'], y_val_pred[0][:,1])\n",
    "auc_seasonal = roc_auc_score(y_val['seasonal_vaccine'], y_val_pred[1][:,1])\n",
    "print(\"ROC AUC H1N1:\", auc_h1n1)\n",
    "print(\"ROC AUC Seasonal:\", auc_seasonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Preparation\n",
    "Creates a properly formatted CSV for submission. Probabilities reflect likelihood of vaccination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only feature columns used for training (drop target and ID)\n",
    "feature_cols = train_encoded.drop(['respondent_id', 'h1n1_vaccine', 'seasonal_vaccine'], axis=1).columns\n",
    "\n",
    "# Ensure X_test_final has exactly the same columns\n",
    "X_test_final = test_encoded[feature_cols]\n",
    "\n",
    "# Now make predictions\n",
    "y_test_pred = model.predict_proba(X_test_final)\n",
    "\n",
    "# Prepare submission\n",
    "submission = pd.DataFrame({\n",
    "    'respondent_id': test_features['respondent_id'],  # keep ID for submission\n",
    "    'h1n1_vaccine': y_test_pred[0][:,1],\n",
    "    'seasonal_vaccine': y_test_pred[1][:,1]\n",
    "})\n",
    "\n",
    "submission.to_csv('h1n1_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()   # shows the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.tail()   # shows the last 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.shape    # check the number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back the CSV to verify\n",
    "pd.read_csv('h1n1_submission.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Plots distribution of predicted probabilities → shows how certain the model is for each respondent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# h1n1_vaccine probabilities\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(submission['h1n1_vaccine'], bins=20, kde=True, color='skyblue')\n",
    "plt.title('Predicted Probabilities - H1N1 Vaccine')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# seasonal_vaccine probabilities\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(submission['seasonal_vaccine'], bins=20, kde=True, color='salmon')\n",
    "plt.title('Predicted Probabilities - Seasonal Vaccine')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
